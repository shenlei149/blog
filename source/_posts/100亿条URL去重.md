---
title: 100亿条URL去重
date: 2014-09-19 14:05:39
tags: [面试题]
categories:
 - 计算机
 - 算法
---
假定给你100亿条URL，如何去除重复文档呢？这里重复的意思是指URL一样。

首先，我们估算下100亿条数据的体积。假设每个URL有100个字符，每个字符使用2个字节，那么存储这100亿条数据，需要2T的空间。我们或许没办法把所有的数据都放到内存里面。

我们先做一个简单的题目，假设URL条数很少，内存能放下，该如何去重呢。我们可以建立一个哈希表，把URL做哈希运算放到这个哈希表中。如果key，哈希之后的值，已经在哈希表中了，那么这条URL已经存在了。如果考虑到哈希碰撞的话，比较下两个字符串就好，也不是什么大事。另外可选的方案是排序去重，这花费的时间更多，而且没有太多额外的好处。

回到题目，2T的数据，怎么办呢？  
方案1：  
假设所有数据都在一台机器上。首先，我们可以把这2T的数据划分成2000块，这样每块就是1G的大小了。一个容易的划分方式是Hash(URL) % 2000，这样，URL一样的话肯定会被放入同一个文件中。从概率上说，不会每块都是1G大小，如果是随机数据的话，假定满足正态分布或者泊松分布，超过2G的概率就很小了。再说，现代机器能够使用的内存很大，这都不是事。这样，我们每块都能放入内存中了，采用之前说的方案进行去重。

方案2：  
假设把数据在多台机器上。处理的方式基本上是一致的。放到某个文件变成了发送URL到某台机器上。

多台机器的优势：速度快，2000台机器可以并行计算。  
多台机器的劣势：我们要依赖于不同的机器的稳定性，如果数据更大，机器也会更多，处理错误也更复杂。也就是说，增加了整个系统的复杂性。